{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Deep Convolutional Generative Adversarial Network (cDCGAN)\n",
    "## Overview\n",
    "This Notebook is a simple implementation of a Conditional Deep Convolutional Generative Adversarial Network (cDCGAN) using PyTorch on Shoe Dataset (Gray Scale) found on kaggle (https://www.kaggle.com/datasets/hasibalmuzdadid/shoe-vs-sandal-vs-boot-dataset-15k-images). The cDCGAN is a GAN that uses the label information to generate images of a specific class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Importing Libraries](#Imports)\n",
    "2. [EDA](#EDA)\n",
    "3. [Dataset and Dataloader](#Dataset_and_Dataloader)\n",
    "4. [Model](#Model)\n",
    "5. [Training](#Training)\n",
    "6. [Results](#Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "from scipy import linalg\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "import torchvision.models as models\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28fc99c9470>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../Shoes-Dataset-Colored/\"\n",
    "BOOT_DIR = os.path.join(DATA_DIR, 'Boot')\n",
    "SANDAL_DIR = os.path.join(DATA_DIR, 'Sandal')\n",
    "SHOE_DIR = os.path.join(DATA_DIR, 'Shoe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 136, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image size\n",
    "img = Image.open(os.path.join(BOOT_DIR, os.listdir(BOOT_DIR)[0]))\n",
    "img = np.array(img)\n",
    "img.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"../Shoes-Dataset-Colored\"\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Boot': 0, 'Sandal': 1, 'Shoe': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert images to grayscale\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Assuming grayscale, only one channel\n",
    "])\n",
    "\n",
    "footwear = datasets.ImageFolder(root=dir, transform=transform)\n",
    "mapping = footwear.class_to_idx\n",
    "mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 32222\n",
       "    Root location: ../Shoes-Dataset-Colored\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               Grayscale(num_output_channels=3)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.5], std=[0.5])\n",
       "           )"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "footwear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(footwear, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is Based on the Paper https://arxiv.org/pdf/1511.06434.pdf from this video: https://youtu.be/IZtv9s_Wx9I?si=Hw91zHv2KjFU-xel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deconvolution Equation"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAABXCAIAAAAbC2PKAAAgAElEQVR4Ae19B3gV6Xnu5Ca5T8qT2E42ydqOr2/Wu3GSTVl77ayv105s75rdeAtbYAGBJEQREqIICQkQRVTREb0X0XtHdNF7XyF6F12AhJBQOZLOe3n/mTMz50hHHUbmfPPwwJyZv77/93/9HzTIJQgIAoKAICAICAIvHAHthfcoHQoCgoAgIAgIAoIARAALEQgCgoAgIAgIAg4gIALYAdClS0FAEBAEBAFBQASw0IAgIAgIAoKAIOAAAiKAHQBduhQEBAFBQBAQBEQACw0IAoKAICAICAIOICAC2AHQpUtBQBAQBAQBQUAEsNCAICAICAKCgCDgAAIigB0AXboUBAQBQUAQEAREAAsNCAKCgCAgCAgCDiAgAtgB0KVLQUAQEAQEAUFABLDQgCAgCAgCgoAg4AACIoAdAF26FAQEAUFAEBAERAALDQgCgoAgIAgIAg4gIALYAdClS0FAEBAEBAFBQASw0IAgIAgIAoKAIOAAAiKAHQBduhQEBAFBQBAQBEQACw0IAoKAICAICAIOICAC2AHQpUtBQBAQBAQBQUAEsNCAICAICAKCgCDgAAIigB0AXboUBAQBQUAQEAREAAsNCAKCgCAgCAgCDiAgAtgB0KVLQUAQEAQEAUFABLDQgCAgCAgCgoAg4AACIoAdAF26FAQEAUFAEBAERAALDQgCgoAgIAgIAg4gIALYAdClS0FAEBAEBAFBQASw0IAgIAgIAoKAIOAAAiKAHQBduhQEBAFBQBAQBEQACw0IAoKAICAICAIOICAC2AHQpUtBQBAQBAQBQUAEsNCAICAICAKCgCDgAAIigB0AXboUBKqPQElZ9cu+VCXdlc6m8reVVpWXFSBQGqhkVgEWL/CRCGBfsO9lI2Iy1u33fd5wfhe6rLFcuIndX6Ps94EbHTyL4xetkTe0u9IyPMh1YFB5hdhyFFmPK+569yloXfA4v+K3L/HTEcvw7gC/8yty4cMhmL/VKvAwF5uPIq/AetIw74pcMDdr91nYeqRBDDNqClqPq/FIrt3FtuMoKa1xxRdW4b6fbfXCBlBlR84L4FsPcfFWleOsWYGbWbh8u2ZVzNKTV0NrA62D+aAB3bhKETcT2hfGkHLyoLXiz2PnXtAg84tq2VHGVWhNoP0W2Q1VlkxbD605jpz1O8Hcpzjk/629WnFpDVSinrO4iG2S7Q1Y9+3HQovAgwbJR/alo8imC1qDrvNdmRtaC2jd/DZ0+gq0YExcZRV4tz+fDF9kPWmAd6v2QGuNExc4tMu3oTXFhBXOD/NBLim/xagaj0T7jMu08WCNK76YClfuQHsfw5a8mN5q2YvDAtgNfDAAb/aoTzWqzI3/6ouf964BE7SDl/0ECXNw9Y79WYO4zyvAp0OhhWLTYWM8rlK0n4Dmo/C0tnKxRhNbnEbVpHbWWE4e/iIe/ebXqMPnWLikDBsO4X621UV+IX7Wl/zxnu0hgI6Tsf80i72biGbVY1JaBAYvsFqu/G7LEWhdcbgiFSq/AH/QDaGjK2+g/t8WleB8Jo6ex51Hvo1vPIgkxdFmbYT2FZ489S1QL7/P3yCdT1/vt7GF27hSp2wOlXlboEXjUn2r8n5HUPMX41dDa4m+KQZfSjsCLQxbPHu55u3VW420Y9DaYdG2GjcYPR0/7VdLhlDjzqpR4dgFHDvvVW74UmK+/oDXw7r8yCvEqr0oqD9+67AAvp8NrS0GzasLJr517zygnBi+2Pe5v99FJXD9PsQ/Gg2mjr/Re8e6YXm0/E2wXp67gS+H4vtxtSe+hhPLPHQWvx4ILQQpm7ywufsIWkdETPB6uPMEtEh0mkKDLOM6vej5hV4FfH5cuEGLdnaqz+PKfhaVVPz2yi0OcpHN0VpxuXp9uj8D3+0BLYicS2tnaXt6J4/zKTZ6zeG2Xb0fFzKReb9eu1eNbdjHiadf9ttyu3HcCz4U5WrAvtClOzjgLlOsGU1dyzmevWY9cepu3AqqO+eu16b/UtOfXpva9Vbn5gP0mE2a1GJ92wxL5qZ+WB/RpdRD+Nt4Gv370317qfXvehPAj/Ox6xRSD/lqzRdvoZLw/rajFJapB5GTj0u3DeMjZgZGLjNmdC8bo1dgzEoWMK+Lt1DmX2SmHmSbW44Ybd7PMev53mRmoc9cxti0cNx9yLduWNG4/CJ62Nygxlro4nOfQN2edLSbiKgpvjaTTzdlbhQUw1XKP08K2Ig/hutT0f5z10lu4Gnr7M9qdp9+BUt2WNL6/E3sz/ByIVayTAC9oFp79JvDTkvdla0pgDPXMWQxQ+k37lU9SJ8AdnEJCWn9weeiWZ+4hKYjyPj+rSddu3YTSh/opNVkRieVaZVXiNX7MDMV34nD92LxHz0VqbTBnq+NSRW6SPDR0zHcQ64A5m2miDp/g2VKygzAXTandKkbj6tnOG4+REF46aYXhm7QNl2zD5lZXs/r/qPQhdiZ3Ds/6Ek9L+0E3uzJn+fUXABmG8zdTF/lN6Lxrz3wSiw3ToIiicJiTFprbYScfOxNr3iE2Xk4egG5lSLQL4VegUoUHa0F2tc8ZlkhRPlFvoK8wmL6Fq7wVZUPs59QBnw0xKujgXOJnl0wPCmA21uenb7Khb7gTQDFNnUt9ym5SvWv4hIcyMDa/V6sLGw0/iymMrSf+YGyHmNGKsInY+vRqnvz2dEAvr7CrfQ8/BN3szF0CXf0N2PxJ9EYZ9uJ+kDPXacqOWpp1cMGvKSMWcEN7oWf9qWAJxPo6OU5M4vV7qZ+BPDxiwyaUmsOIf8641Gm9n4N7UNs9hhtBUVISEGmYsohyWgyEv+RgB/GQevEIIT2MQbNJx1o7dA4idPZcZL7XwuDFoWmwwzq3HmcbW4/Zsw3vxA9Z+P2A/5sNYZt/lsv1WaUavMjv2Gh1EMUaVoHxM7AB0NwWwng7cc4jMMZbE1rCi0GvxmE1xKgdVYTDEFOntEvJXcQ3htEx1eIfydhxlV8s4eq3o0ltQjio3VFln+1wOjA9k+ZG78dSAepj8ofPhHDPISVdhwpW4w6mVk4ct5LuBKcYQQzNx/FJRiwAFoz0tOHQ4xieYWULkt3WL0ePY9bCtW96VichoHz8cfR+CwJQWPwNz1YuMIrvwgdp6jIdDjh1f1aufloNIRqFoDZmxA9DbM3Y8p6DFyA0LH4p9647PEcXr6N13uphWuDd/vVjLlUOB77wzmbSTm/HoiTl6jKaO0riK2SVwaj92zWSzuGsHEYsVzJ3Wh8NRK5BZbydPIy1W2aiZ2gfUphs3IPFqXhg4GU1m3H4/3BpJnU/VS/tC5IVtG+G/fxTiKR7zuXXez9Gj8b4MWFS8rYTuuxGLwQcTPo5rWrmrlP6QbnZglj4zWiIjsUFd5vP8qJBCdzwPq1bh9RWqhM8Mf5eK0npm7A9+Px57Gc0dlMFHu4/+7jHOqaPay365TaLKGktxM2LzGA+du4u7UgNBpk7DhzJI+fYtIaBI3G1PX4oD/Cy8nXCzdJMCHJ4Dq2w9I0sypW7MZXo4wMrFsP0H8+CtUU8osYtvengmdcI/lpHfFGAjKuWq1VeBeajOm1VX9np3LJfLJS2ozBfyag2BNEn7mRzHObh1UWlyJiogqEt+bf1+4agxq+BK/2YC1XKQYtJA5/FEf6uXEPQxbBpWSznqxgF+165TsP8c+9VZtt6NTRJffTInwjFu39pCDoFeduIRmQcUXgk0HGSEYsNWj4zDU0H03CmJGKEUvReQp+1A/LPJykyEUNlRQbSqK96K1MGG3V9p9rd6H9hiu4YjddUwSwoqS2xkkMWFRprJ+/TnDKR1W6TIH2OUnl8h3ETMU7CWSh9XXVgwC+cY+q3H8l4swNHDxD6fLhQGO2c1I5czNFaNNB7lJdvHWfhcbD8N0YvNULC7dj72lcU4I5O1exp1k4doEiMGkxsnLx2RDK4EdPOGu6bmzhn7X7WOy4cv1HT8enqs2f9CYr3Hca1+9bNp8dsut3SQ3NR/gmvo5ZzsbPKL/Q5iPoPZeW3PKdjHtpHTDWkzExexNlzLQNbDJ+Jhol+o03Hz6Dj4ciegZGLieBDl2C5FX4aKhlK9hH5e/++HlS8O6TXu8f5lKINvLsB+6QMBbYelRx53ZoOdoalRtoloRXY2l7fTGM22ntfrQdSxZ5Wjn60q9whyR79McHOeSSo9XPEFXs1Ri81h2/7I+oqZi31bKKvMYE1WYHzNpIE+fRE2MAR88SrkmrWZaxuhiGNn+WiNBkvBLDldWTjJgMEk55du0elu+gbMvyDsf69FXTnzl5yHxg0EOXCfhZH4Nh+bTTcSJHa15JC9EkiYSkBRlTAHDtDjF8uzeNuUIXFWci1opzeb07fhBP2h6ymCAXuZjYpYViwRYW07rgR73RcgyJ9mkhukxms6YXt7gUHyWxcMRk/KIf/jAavxtsDoSS+J2+XNwDGZQrWgscOWO9rfudqxRHznkpeUMWcjvYE9MOneE0M7Pw8WC8lWA5QjYeIC3tO4WNh0iW/edj7lZyxrjp1rgmrmHdxHlYv58L3c4mYslDYli+2wylp3bBTO8AcNoJKkZv9qLDidpwBDKuWC1/mURRlK34w4hFHMCdB0T1e/FkJlo7L4NPrzZiGQns82FKjEWhg20wVru2u7f7YKxnd9geV33rKiWG3ad5lXSV4lf9EDmWD0vKED+Li/6PvQxV7JkfjkQYhnUHVK5WKLaoKCaNk2j8TiWHd51KwDtPY8WdxzB4PvHJyqEuSyTbEmF7ukZOHv4qjtBlXGfalPYVbiihTrpthwkrvYZn/8HEsVBETqQ6nl+EPBV/ySvgPn2vH3fThUzqBFpX/DABwcn4cQJ733HcaCMkmQNLPaQmEoS1e+1t1/W+tIx+U/1USNpR7ogrHlXe3jRtthY4bSMY+1vz/lgGZ5qjqMh8CNCnq+twZW78vz7oMsn+sq739SCAP1GZQbq1BKDHTEs3jxhPjmO6kuj9C7X8b7cfUmeZusZrDg9zSFihY/FqDCavNV61HYM/9ThJwsaQwsww+AeD2abZxY37bHOWEo1e7Xr/YOZLG8wqF6jjCIN8daXsPBJuK4+Zm3mfdTupZTh7g4x49kbv1uv7V59Z3CR2SwjA2euc+AJl9Za6OYz/7s/cMS0UHSYoQdjaijC5StFoABoPQf8FFBLXVdxu7iayBl0Ar95F3PT8TEBt0VDsU77WUjeyn3JN/6hcfMVnog+yyWF1ZOyvVu/mqEw9rNRtcPmUzUT7a8/GCB/HXq6o9LceM/HDHhULSLPl/CK69POLSFEPcw3WYL6t5MZVil/08ZIN9sIMi7QwfMgXMqG9g3Q1Qp4ICjcKpmwklzmkPCVm3fwiem61tr5chnakirF9mkRCylVHZc5nst734ljevDpM5EIfUQlZ5zNJZmOXmy+V1R5Ku5ym5Ba2aU8is8p57opKqAY9LSY+j54g2+O88byv4t8j50hUTUdYKmxxCSXfOMWsqf91sQya5Tu5djM3U/jNUxYzCwSTG+gXd1wIEpRrITef0zTl2dMiil6tm8Hm1u7lxHXa0+ueukS6ip5mkE3URBpwZmZ4QRHeiEfz4UZHfVM4khtZNOy+H4+kRSzsY7x2nUbpO2czq2QpXdNuTxsNef/TIdmYl/fjqn8dSKf27MP9856SEsYs44zIP1tiwXaL7RCBECzazsaJ22e4os50ENJwCks+bIrdp1jg2l3astFTuRw3H3B3/7g3/VVahFdeVecpbFP3TQ5eoLicYihMWQjGun1+J0LlpqNvmuczN4PWFsMWWrV0e5pMKcLi26T8VsZpMRoGzYxtZVXzvnOVkVafFnOPZOcZFpd3Eb+/GMmOrMB+BfAol9qMj6Ap31DGNVJOJdbtoyfcEXO9E0fKt1OjJ3UVwFdvc/36K3+a3nFiCoHIyaMX6C9j0dWTd8AMF+U3M8fHEFeoLxejeAvH33bDZ0ONgiS7TohQKqquA/aYYbwiHYSR8sxrvWJ2ukFsPix/w13XhYQ1YIGX3/+ZRfilZyfrtR7k4pU4fJpk+eWmrSX72Hua9s0zf3vTkeWb93pyN5tq2tW71AFPXcHpazVmhVpkBdJi22ESty4+qWWHMu+syTDq0QCOnuOT9Z59VeiiwUdvvy1AQPMrHA/VEZeEWQTEdL/8ZgCBvaPc8noEqDpxFK5OVyrpC9MsbgIwCqAF+Z7RXLaLB5PMjO6LmdQGxq2kj5dHrZox0lnJNXsjNwMZdxS3DVX+9lZgopKKjPTkkUT9af037nK0acoZWFpmxVMY/PYoQTuOc7SfJHE17RedjcG0j+3X4q2EZf52ypWrHl8iQL6pBaOnR0TRdgy2fHdr9pLM1npW8Ili2aGjUeLGdOXSXGzzwdq70+/3pStYIpSHPEJh1Qbjbed2ylexP6FmGU5sfc5GX7plOeFNNAByJUZYOtNzrl+HzxIiXbBRxLYmCLobkHltYdjkyU2ltLApi7HT+dZ0urrBZrVOhsu0sBg/6oW3E6wIC224EMs5ETuNumbyCpLEtbvK/9/JSymcs4m7YJZHaZ6xnt2ZQRA7CAxDHKeJPGo5ftmXTCB+FjpPJX3q3jifwhX+pGDoQm+H/SL/Ccfo5Wg2krAc8SZ1EnYMFe41+6gLjvCEma7fIwnRWR3tdQYaoAX/eRKGK0fdnYfcyFo79PKQFnlvELWfEreKmDZjUFO/0o4Tjf2etAb7IPX7vnNISM8mbp8y2Uswdnsa0Uvy2E8r4qNfxSX4SW86cgpdjKpowZxvJReFd0dFtJ0oxQlCG0R6xEclFfVXnSfh/cSKT9OUlPJVp4kVt5FXgH7zmGkUPQ1/HYOY6eg2nWcf9JCZvc6lm5xgWjWi4PZald/XVQCv2sXNo3uVAW7Ot3rhdwPZ6e0HXPUVO40BRE4kqb3S0xrPgHkkfT34aj49q9QQrYvF/QcuoIao+8HoqmphWRhtxrLNtxPN2qAgaVstB++9bHSZyq2rRdBDAtBjowVhrM0bQ59hLP41wZK+AL2CryvDRQtl2LVMJSX505syrkB7j+JE+0r9aa7uvzQEpzXuSu+0FkgpZ9OPX0kafaSy+zjyUHw4mHLlpsrNoXehlZWS8LQQ/6RC0VM8cSweQ2xneMZcpXi3j+WF25+hmG8IVVH92v8126/OkYnLt/Gjvuz6J31xwOMgfX8AGYr9WraTev1a29dOFm0jk/1ymBKl0VWfuL14k8J7x0kczCCXTDtBxmQmH9j7Kn9/9wFHOFcZQOXfUuFrjwVV5R4v3UkAKUHnWJkBjQaRhHyiTSOXMntc62rYNGaPjGC1xHKPHCXTiTHMTTpdormahz0YnrjARfmfQSyjhZKjVX7dzyY+acdx4DSDstuPMyiz05tj+mthtbJB3+hh0JK/YvbnU9eoAccYYhJAyCgO+K46yER7PYwBb51FaN04Nd3Qp2xoi74qkwtgeF4PppjsnpHpVtju4Xr0Y0ch1KYln7xIGM0ATZvRDA1onbFB5RyUufHpIPx3Xys+ytQNxaAA5jeR6ccg388XPDYcpOwZt4qzazcOo1aQWbebUK3sQh2fpPns3edrFcxvjzAUi8+Gejm3GB1ri/cH4B96kmPMtdHhJaUV/Wcf2rj2q8yNJkPUIbqO2KkCVcUl9PF8McTIm0k9wM3VdLjCNpzGg3mtVh7m8tmIZoFCF7rPIMlpEZjlMf70CJH9rCalbzgDwCbx6xN5rz/eUFkdU71jCmb75s3jfGw+QkLdf5p5jtuOMbK7qnou6zI3PhtMFcTf1TaZxol3iptRNjefSsPo5XQcfr87FZShSygayh9kYEpTME54tqS/vmr0vK4CeNIaCmA9aApAD/PoXjIelm+Bw+rbBWNWcP0Yje9uDa9tMrmSfoY196nBPQ+eJpWYG5JmQSuqnPrFzdbCyOwYtpSbp8tUZhaYV+hI/HGskYKRk+83VGmWv3CTnlutGfW7czcoj5coz4+eEfdOP3w8xPAZPsv20j87wCyhKKYQ6yMvcjGtppLkwB0nKR52ncKedNbao3Ka7OEZczD+brS2GGDzMejFoibh77sbrnh6SjtwVMs86g4/ZdCc54b16+Fj7sDX4j1jLqFWYWo/1Jc7Yrpy+GdmseRHQ7wEScomImOm41Z+7LjUzY30ejyJ9YJytGohlj8D4AFcama7vaY7YQX5UeREig1dm9mbjpnl1A6vOrX9wYB3a+YWnctkWlDHyZjq0UsAZT10rCA8Ub63rMfMUdA64p+V14GJe2H43/FGwdIyg+n3n4e/iyGAPqfdDmbw4TYlWiiHmlvHolqPpUtWi8AZj39+0yF29OUwprLrJwKu3UWI94mp8iOs6ZMb99FmPGlp8EJLlFankd7KTjJTIxlcCGbCnX61HEEvsS6EOk1WorqjcWSTDv9gI/aRk4fvdMcbPQiL+cmX/ikkSD3GRPEcSm0mwpY0RKdXsOHjLVKenle6ockIa9RdJxJJ3buzYieX/qhy8l+7R0i/HYvGgyzfhlXN+077xJLx3m+q+DVyCb5lC5nppfVwxqD5GLWM7K6bLVJ+4iJH+9uBpEz9SPqjJwifyBGevaq2eaSXfx6gef2DePxlN4Tb6CF8tLK81YnVGRu4uULG0NLQN+/JSxihQtrL00i0lQhgfcDnMpWx3hYz1JYcopzYpsPsZhYHZg/qA2DgIByNBtLVr3+RKusxPq/KX1gFmn5eF5fwqELrMQyubTiIvvOYBWn/SkzMFHwy2MqJqbCZ1bvxPzZbrnyZdXupG6WdwMnLTAAKGWts3vIlq/+krgJ4uwqmTlzN+M2IpWSs0Z50g5v3qZn2m4s+KYxY7EtXmTUtLDUkaARlxvEL1Hq0DpisgsHUyFTO86krmKCOrkdMsvzy1++wiwHzmfnMfK6zDIJqIdZ8v1JhthMXsVWdLi8f5dWLXr+Hh08odNOvKsXwK0bLTp6j00NP3C0oxjv9SLW95yJqMl6JV+ZOR2ZLJs4hYe06yejRtuP4mzj8ZoAVk7aGUn93v0r01XkBtByGt3qiSBmp+9PJSr4b59XlD3tx/FPXkzfxbHR7/LofHuXhxn3mnmihXBH9YkpFZySm0KGqRTBLi3lDX1jux3HLuUtPX6VNPHU9s2Er1CXt3adsIafbdlg5WoOYoK5fTD5vh05TabxOXoeYGUyrOX2VnhItzBhSfhHFjPYx9qgol73ZutyXlDEKsPkIoiYxUZkxiM+JW4vRpEDzYoJVMDZ6HKTmc/uNxxWN/GJmk2mfcW8XlZAU9ZOIV+/iH3phg2okcR77irUxWb0p2iWhxplC5s4E0y+a85RpfVok0tQRgAVbkXGNXqKvlezRv01RUqYyjVtyg9TjtXwXieTPuiJkPGXnuJVMS166k59lTUipwubrkEzK2XmCFMKk2ebWmdciFxNzftmX6k778VxlJqyGGRm/q3bzfv9p5vj8SwKaDFcpWq1pHx/M4Mbvpk5g38uh8qoF0/UaOx0/7sVcAf0DKXM3EsZLStXLeUKdXovEdZurn+Ingl9ESj2oUjg70BUxbjUjIJPXo8lQdBhfBYp5RdAaccvX4lqaRhPCJ52QXMtjt7QZZ8jgPCUs6eQLJtvUT0Uy264T+qXw59kr5DxajBUq0sdzL5s8U+tkeQ0BHrzRIuk833IY/OiHOu2peyDoEP7ScLHO3EChctS/VWeSOplDFM1Efp1mPEldl+V0tsXQ67DlGFMs+8+nh2D+NuUBDUZ/9Y2HMqhFj2Csun6v2w9JGKOW03v8WncKGq01fjWQvMX0OjzbL79KrCCv3mck41fgC49rxP6qoJi22Zp9lBRvxJGWtC+UR2R81cnz9nYqvK+rAC5zU/2nLy6MM5+w2os1U0yGkcftUpyUbrQmFtdYor6sxINGwcyh16/kpTxf+L0epFotguEu++UGtx/bjGKSM0AZrH1JqaBfTLJtqwqEGCqevbp+T3PwA1InqTkYfxfHJeRhtYtcvyELyUlpAkbSCmkygvJ+zhbmML/eiw498soIEjSHEcwTI+axjfJ91cuTXjOJht3L7Xaj8QCOX3dBM2ujia86RtsiEm/3ZXYoYzaRijd1VMPuSl+f/YqfrhaxFdN384vU5mlmhQwZrW+Lb8VyKbXWvgcTzXaajWZWdqsx+Kvu7OWjJOaYPCmgqvRNlcC1Uz9X1tngF+/0w/uDqHt1n05fJamoE/hNwSiqWfX4/RojvtBVCchQnnPVIpmDfetRBXYe+XWLyuQNWWRntEpWGcsdOOxluwwMvh3PV/8njoTRZbKRNMSPm7a1YjEmXGuV92+JckGXlLEKTzS15/SfnVBnYl2osnha4uptOnW+FcdhUy+MoYQb48nJNxusyw0FYXsOnsy0D36RiDcTlI7SRg2jfWXpM0UuZoe+Gqv2RSQnMt6bD/SZbey1NxOYKFTgybrKymEaF8PDnbg0zUYyXshsAHWESf8c7KLtBmFoQUhW4aGFW9WTIPRVKV2z17Gu7oKmKdbcCqvrgJC0OhO64xeojHLbhnF2B85wT2nd/J5UNPEsLuXptdodY6Vy3AqHvCVc7FRSuB7nLnKpL9x14PbsrrS0/xtPJH/cB38fx2JmVJWO69ZoWs7Reu02eWCSJ/quD5sB4yj+Sb+igmsh3LzcXF1oF+mJcgAWbCZxblIxOHO+5s3qPfjreB7G+3mi4nhRBgg8SRGEW1m0YajxdFF9ReL1nvhgMDmq1hiFJUyS0CLwVm98O457Kn6m5aA2u6jLDRljsPoTrTxGUbRNzcCZ2TJprBXzFiu/Vu+hIPe5WFdJKC0M/6Ik3dBFPJhjT4DwqVKjn3UVwHSAqOTPtfuNc0Q+3V+5Y3mTADoYzdQegKG+tfu9MlM+H4qf9mZG65lrfk/rX7ljKF+6kph6CHdt51UOnGGb1yv9BMT0DYy6L0rjYSfzJLsbPPCa9Zh6zXd6UuL6JI6WeT47decRVu3B6mOiQIEAAAZ4SURBVL1+RZEPCHX8yTMewYbfzGxq6Q70TjG0vPxCDsbU+MwyTwoNfYj2ZQg5zsxUBlfK+5BdpfQJ7zxl6U9jV3rFQWer87vT1nvlYpgd6TcxM/C/4vC7odSCj5y3moqbjZTNyrXbBH/anYcr7mZbWsuDx4arLf0qMyDajKOK5nNo0qejWvwsLaNVrZ/rcJVx4eyfdjEbpCAMx797XMrmc/vNnYf4Rg8GPlol01LMVKel9QI3szBwISMm2zzn1AGKrncHepG9Xvj2A2Kle+n1r5cMW8JV0E96QBkNszZaCQ3PTI3+83lAIGkx7Yn6vRoPp9jbfpwBF/1DCm5w597I4mcpn/gJkepjYNS8A934q/cSZD2P3T68IheDeTtPGRoJQKs6cZ5hye04yePsW49ZBJN6iEFunUrdYOhx7CojF10/tzMjFTs88eyz1wmj7q11g/+zRfnk8EKXcViFuSkPyXB0w455KsFYXPOvMNpnV/k9v2DTHEO8P006ZR0aD7fmW+hiDDJqMtkRs3afUBKEjKUdfNAmuYtcfKgnrNg7LXPzoX4Ky/68oNjS2s9epxsjbBxbtn97/9h5ImBGl+zVeabxCD06P++PyMnsQk/g1886ho3nKfAWo0g2ySu56HkeblPownl13jc7D8OXkmL7z2Pord6vQ2doap+8hAIX95e/T9MwMb6F7xeiqjmYvALu8W3HqDgWl9AUtv9fONVspJJi9SCAK2m9Fq+0Vogv56yrRTt1rFL+Yy51bLAu1anXh1X2uY8qG2dqepMqS9VDAfPbTxW2lfW4AtlfYUmnHi7eTmX5gC1LpcKRlFX1LbAKazXkh4/zGZSp3cXEySAcfA4ctnbjqX4tCphWxlcEql+rpiXf6Ekr09/3QGraWr2Xt8dKyzdeWlZZ6DS/sIIz1uUbcfAJk1paMgzaMK+GJYDp3GuFWR5/csOEzJFRMRweRuuqdhddcLb0t9o18tLX4inqtsZZ1Zd+svU4wZW7aEWZX6ysx5brsSnGlZJ8D/y8N4AMx/yeVz12Z28q/TLdmO8men0Zw15A7p8TAq5SHh7TuvlGzZ9Td7VotmEJ4N3qgGDl+S+1mORLUMUNfmVTa4LlnnBj9SfFMy3B6FqvH3Cpfu+/RyV/mUgt53lH9H+PAKnmULtPJ4HpB8qrWcWRYvwfNYL4icdDZ+la59GM0OeVae8zQSYVBuEX/X0ey8/niwBPyX7kG4B/vl3WsPWGJYD5hd7mz90jVEOIGkrxklJ+aMLn2HR1BkcFvLX1Hc3qVAnMMicvVZZqFJiYVGfW9K94f7O6OrUcKbNAT+lqST6jBfEjG1Xm89fXOI+eN0K89dWgtFMlAvezfVNTq6zyggs0LAF89jrPpVQek3jBAL0E3dGzGmJ90uElmJFMoUEh8P4ghFT1LeWGM+AHuTwCt2Yfz3fJJQg4i0DDEsDOYvES915Yf/99x0uMkkytdgjwv+ysXU2pJQgENgIigAN7/WX2goAgIAgIAg4hIALYIeClW0FAEBAEBIHARkAEcGCvv8xeEBAEBAFBwCEERAA7BLx0KwgIAoKAIBDYCIgADuz1l9kLAoKAICAIOISACGCHgJduBQFBQBAQBAIbARHAgb3+MntBQBAQBAQBhxAQAewQ8NKtICAICAKCQGAjIAI4sNdfZi8ICAKCgCDgEAIigB0CXroVBAQBQUAQCGwERAAH9vrL7AUBQUAQEAQcQkAEsEPAS7eCgCAgCAgCgY2ACODAXn+ZvSAgCAgCgoBDCIgAdgh46VYQEAQEAUEgsBEQARzY6y+zFwQEAUFAEHAIARHADgEv3QoCgoAgIAgENgIigAN7/WX2goAgIAgIAg4hIALYIeClW0FAEBAEBIHARkAEcGCvv8xeEBAEBAFBwCEERAA7BLx0KwgIAoKAIBDYCIgADuz1l9kLAoKAICAIOISACGCHgJduBQFBQBAQBAIbARHAgb3+MntBQBAQBAQBhxAQAewQ8NKtICAICAKCQGAjIAI4sNdfZi8ICAKCgCDgEAIigB0CXroVBAQBQUAQCGwERAAH9vrL7AUBQUAQEAQcQkAEsEPAS7eCgCAgCAgCgY2ACODAXn+ZvSAgCAgCgoBDCIgAdgh46VYQEAQEAUEgsBEQARzY6y+zFwQEAUFAEHAIARHADgEv3QoCgoAgIAgENgIigAN7/WX2goAgIAgIAg4h8P8BXYFhI4JK3y8AAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g,num_classes,img_size,embed_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.generator = nn.Sequential(\n",
    "            # Input: N x (z_dim * 2) x 1x1\n",
    "            self._block(z_dim * 2, features_g * 32, 4, 1, 0),  # N x features_g * 32 x 4x4\n",
    "            self._block(features_g * 32, features_g * 16, 4, 2, 1),  # N x features_g * 16 x 8x8\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # N x features_g * 8 x 16x16\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # N x features_g * 4 x 32x32\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # N x features_g * 2 x 64x64\n",
    "            nn.ConvTranspose2d(features_g * 2, channels_img, kernel_size=4, stride=2, padding=1),  # N x channels_img x 128x128\n",
    "            nn.Tanh()  # [-1, 1]\n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes, embed_size)\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, \n",
    "                out_channels, \n",
    "                kernel_size, \n",
    "                stride, \n",
    "                padding,\n",
    "                bias=False\n",
    "            ), # deconvolution\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x, labels):\n",
    "        # latent vector z: N x z_dim x 1 x 1\n",
    "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
    "        x = torch.cat([x, embedding], dim=1)\n",
    "        return self.generator(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution Equation:"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):   \n",
    "    def __init__(self, channel_imgs, features_d,num_classes, img_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        # Input shape will be N(batch_size) x channel_imgs(1) x 64 x 64\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Conv2d(channel_imgs + 1, features_d, kernel_size=4, stride=2, padding=1),  # 64 x 64\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),  # 32 x 32\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),  # 16 x 16\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),  # 8 x 8\n",
    "            nn.Conv2d(features_d * 8, out_channels=1, kernel_size=8, stride=2, padding=0),  # 1 x 1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes, img_size*img_size)\n",
    "    \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels, \n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, labels):\n",
    "        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([x, embedding], dim=1) # N x C x H x W\n",
    "        return self.discriminator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    N, in_channels, H, W = 32, 3, 128, 128\n",
    "    num_classes = 3\n",
    "    label = torch.randint(0, num_classes, (N,))\n",
    "    z_dim = 100\n",
    "    x = torch.randn((N, in_channels, H, W))\n",
    "    disc = Discriminator(in_channels, 8, num_classes, H)\n",
    "    intialize_weights(disc)\n",
    "    assert disc(x,label).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
    "    gen = Generator(z_dim, in_channels, 8, num_classes, H, 100)\n",
    "    intialize_weights(gen)\n",
    "    z = torch.randn((N, z_dim, 1, 1))\n",
    "    assert gen(z,label).shape == (N, in_channels, H, W), \"Generator test failed\"\n",
    "    print(\"All tests passed\")\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "# Set the device (CPU or GPU)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "lr = 2e-4\n",
    "batch_size = 64\n",
    "image_size = 128\n",
    "in_channels = 3\n",
    "z_dim = 100\n",
    "epochs = 500\n",
    "features_disc = 64\n",
    "features_gen = 64\n",
    "num_classes = 3\n",
    "gen_embed_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim, in_channels, features_gen,num_classes,image_size,gen_embed_size).to(device)\n",
    "disc = Discriminator(in_channels, features_disc,num_classes,image_size ).to(device)\n",
    "intialize_weights(gen)\n",
    "intialize_weights(disc)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(32, z_dim, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "write_fake = SummaryWriter(f\"logs/fake\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (generator): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(200, 2048, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (5): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): Tanh()\n",
       "  )\n",
       "  (embed): Embedding(3, 100)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (discriminator): Sequential(\n",
       "    (0): Conv2d(4, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Conv2d(512, 1, kernel_size=(8, 8), stride=(2, 2))\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       "  (embed): Embedding(3, 16384)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FID Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahksa.LAPTOP-394KBUA4\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ahksa.LAPTOP-394KBUA4\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class InceptionV3(nn.Module):\n",
    "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
    "\n",
    "    # Index of default block of inception to return,\n",
    "    # corresponds to output of final average pooling\n",
    "    DEFAULT_BLOCK_INDEX = 3\n",
    "\n",
    "    # Maps feature dimensionality to their output blocks indices\n",
    "    BLOCK_INDEX_BY_DIM = {\n",
    "        64: 0,   # First max pooling features\n",
    "        192: 1,  # Second max pooling featurs\n",
    "        768: 2,  # Pre-aux classifier features\n",
    "        2048: 3  # Final average pooling features\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
    "                 resize_input=True,\n",
    "                 normalize_input=True,\n",
    "                 requires_grad=False):\n",
    "        \n",
    "        super(InceptionV3, self).__init__()\n",
    "\n",
    "        self.resize_input = resize_input\n",
    "        self.normalize_input = normalize_input\n",
    "        self.output_blocks = sorted(output_blocks)\n",
    "        self.last_needed_block = max(output_blocks)\n",
    "\n",
    "        assert self.last_needed_block <= 3, \\\n",
    "            'Last possible output block index is 3'\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        \n",
    "        inception = models.inception_v3(pretrained=True)\n",
    "\n",
    "        # Block 0: input to maxpool1\n",
    "        block0 = [\n",
    "            inception.Conv2d_1a_3x3,\n",
    "            inception.Conv2d_2a_3x3,\n",
    "            inception.Conv2d_2b_3x3,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        ]\n",
    "        self.blocks.append(nn.Sequential(*block0))\n",
    "\n",
    "        # Block 1: maxpool1 to maxpool2\n",
    "        if self.last_needed_block >= 1:\n",
    "            block1 = [\n",
    "                inception.Conv2d_3b_1x1,\n",
    "                inception.Conv2d_4a_3x3,\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block1))\n",
    "\n",
    "        # Block 2: maxpool2 to aux classifier\n",
    "        if self.last_needed_block >= 2:\n",
    "            block2 = [\n",
    "                inception.Mixed_5b,\n",
    "                inception.Mixed_5c,\n",
    "                inception.Mixed_5d,\n",
    "                inception.Mixed_6a,\n",
    "                inception.Mixed_6b,\n",
    "                inception.Mixed_6c,\n",
    "                inception.Mixed_6d,\n",
    "                inception.Mixed_6e,\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block2))\n",
    "\n",
    "        # Block 3: aux classifier to final avgpool\n",
    "        if self.last_needed_block >= 3:\n",
    "            block3 = [\n",
    "                inception.Mixed_7a,\n",
    "                inception.Mixed_7b,\n",
    "                inception.Mixed_7c,\n",
    "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block3))\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"Get Inception feature maps\n",
    "        Parameters\n",
    "        ----------\n",
    "        inp : torch.autograd.Variable\n",
    "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
    "            range (0, 1)\n",
    "        Returns\n",
    "        -------\n",
    "        List of torch.autograd.Variable, corresponding to the selected output\n",
    "        block, sorted ascending by index\n",
    "        \"\"\"\n",
    "        outp = []\n",
    "        x = inp\n",
    "\n",
    "        if self.resize_input:\n",
    "            x = F.interpolate(x,\n",
    "                              size=(299, 299),\n",
    "                              mode='bilinear',\n",
    "                              align_corners=False)\n",
    "\n",
    "        if self.normalize_input:\n",
    "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
    "\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            if idx in self.output_blocks:\n",
    "                outp.append(x)\n",
    "\n",
    "            if idx == self.last_needed_block:\n",
    "                break\n",
    "\n",
    "        return outp\n",
    "    \n",
    "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
    "model = InceptionV3([block_idx])\n",
    "model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_activation_statistics(images,model,batch_size=32, dims=2048,\n",
    "                    cuda=False):\n",
    "    model.eval()\n",
    "    act=np.empty((len(images), dims))\n",
    "    \n",
    "    if cuda:\n",
    "        batch=images.cuda()\n",
    "    else:\n",
    "        batch=images\n",
    "    pred = model(batch)[0]\n",
    "\n",
    "        # If model output is not scalar, apply global spatial average pooling.\n",
    "        # This happens if you choose a dimensionality not equal 2048.\n",
    "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "\n",
    "    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
    "    \n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \\\n",
    "        'Training and test mean vectors have different lengths'\n",
    "    assert sigma1.shape == sigma2.shape, \\\n",
    "        'Training and test covariances have different dimensions'\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    \n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = ('fid calculation produces singular product; '\n",
    "               'adding %s to diagonal of cov estimates') % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return (diff.dot(diff) + np.trace(sigma1) +\n",
    "            np.trace(sigma2) - 2 * tr_covmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fretchet(images_real,images_fake,model):\n",
    "     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n",
    "     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n",
    "    \n",
    "     \"\"\"get fretched distance\"\"\"\n",
    "     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n",
    "     return fid_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_disc = []\n",
    "losses_gen = []\n",
    "fid_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen.load_state_dict(torch.load(\"./cDCGANs-Gray/cDCGANs-Gray-Arch-2-/gen_.pth\"))\n",
    "# disc.load_state_dict(torch.load(\"./cDCGANs-Gray/cDCGANs-Gray-Arch-2-/disc_.pth\"))\n",
    "# opt_disc.load_state_dict(torch.load(\"./cDCGANs-Gray/cDCGANs-Gray-Arch-2-/opt_disc_.pth\"))\n",
    "# opt_gen.load_state_dict(torch.load(\"./cDCGANs-Gray/cDCGANs-Gray-Arch-2-/opt_gen_.pth\"))\n",
    "# loss_disc = torch.load(\"./cDCGANs-Gray/cDCGANs-Gray-Arch-2-/losses_disc.pth\")\n",
    "# loss_gen = torch.load(\"./cDCGANs-Gray/cDCGANs-Gray-Arch-2-/losses_gen.pth\")\n",
    "# fid_scores_prev = torch.load(\"./cDCGANs-Gray/cDCGANs-Gray-Arch-2-/fid_scores.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 351/500:   0%|          | 0/503 [00:00<?, ?it/s, Loss D=2.58e-6, Loss G=-2.21e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [351/500] Batch 0/503 Loss D: 0.0000, loss G: -0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 351/500:  20%|        | 100/503 [00:55<03:39,  1.83it/s, Loss D=5.41e-5, Loss G=-3.84e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [351/500] Batch 100/503 Loss D: 0.0001, loss G: -0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 351/500:  40%|      | 200/503 [01:50<02:45,  1.83it/s, Loss D=4.76e-5, Loss G=-6.79e-6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [351/500] Batch 200/503 Loss D: 0.0000, loss G: -0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 351/500:  60%|    | 300/503 [02:45<01:49,  1.85it/s, Loss D=1.09e-5, Loss G=-1.14e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [351/500] Batch 300/503 Loss D: 0.0000, loss G: -0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 351/500:  80%|  | 400/503 [03:39<00:55,  1.84it/s, Loss D=6.86e-6, Loss G=-7.82e-6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [351/500] Batch 400/503 Loss D: 0.0000, loss G: -0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 351/500:  99%|| 500/503 [04:34<00:01,  1.84it/s, Loss D=4.66e-5, Loss G=-1.9e-5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [351/500] Batch 500/503 Loss D: 0.0000, loss G: -0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 351/500: 100%|| 503/503 [04:36<00:00,  1.82it/s, Loss D=4.82e-6, Loss G=-7.61e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [351/500] FID: 278.4680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 352/500:   0%|          | 0/503 [00:00<?, ?it/s, Loss D=1.26e-5, Loss G=-1.31e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [352/500] Batch 0/503 Loss D: 0.0000, loss G: -0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 352/500:  20%|        | 100/503 [00:55<03:38,  1.85it/s, Loss D=7.87e-6, Loss G=-9.66e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [352/500] Batch 100/503 Loss D: 0.0000, loss G: -0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 352/500:  40%|      | 200/503 [01:50<02:50,  1.78it/s, Loss D=0.00055, Loss G=-0.000736] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [352/500] Batch 200/503 Loss D: 0.0005, loss G: -0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 352/500:  43%|     | 218/503 [02:00<02:41,  1.76it/s, Loss D=0.0006, Loss G=-0.00095]   "
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "step = 0\n",
    "# losses_disc = loss_disc\n",
    "# losses_gen = loss_gen\n",
    "# fid_scores = fid_scores_prev\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    tqdm_train_loader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch_idx, (real, labels) in enumerate(tqdm_train_loader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(batch_size, z_dim, 1, 1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        fake = gen(noise,labels)\n",
    "        \n",
    "        # Train Discriminator: max log(D(real)) + log(1 - D(G(z)))\n",
    "        disc_real = disc(real,labels).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake,labels).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake)/2\n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "            \n",
    "        \n",
    "        ## Train Generator: min -E[disc(gen_fake)]\n",
    "        output = disc(fake, labels).reshape(-1)\n",
    "        loss_gen = -torch.mean(output)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        losses_gen.append(loss_gen.item())\n",
    "        losses_disc.append(loss_disc.item())\n",
    "        # Update progress bar description\n",
    "        tqdm_train_loader.set_postfix({\"Loss D\": loss_disc.item(), \"Loss G\": loss_gen.item()})\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            tqdm_train_loader.write(\n",
    "                f\"Epoch [{epoch+1}/{epochs}] Batch {batch_idx}/{len(train_loader)} Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(noise, labels)\n",
    "                img_grid_real = make_grid(real[:32], normalize=True)\n",
    "                img_grid_fake = make_grid(fake[:32], normalize=True)\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                write_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "                step += 1\n",
    "\n",
    "    # Visualize and print losses after each epoch\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses_disc, label='Disc Loss')\n",
    "    plt.plot(losses_gen, label='Generator Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Epoch {epoch+1}/{epochs}: Disc and Generator Losses')\n",
    "    plt.legend()\n",
    "    os.makedirs(\"cDCGANs-Gray/images/loss\", exist_ok=True)\n",
    "    plt.savefig(f\"cDCGANs-Gray/images/loss/loss{epoch+1}.png\")\n",
    "    plt.close()\n",
    "    # Calculate and visualize FID every epoch\n",
    "    with torch.no_grad():\n",
    "        fake = gen(noise, labels)\n",
    "        fid = calculate_fretchet(real, fake, model)\n",
    "        fid_scores.append(fid)\n",
    "\n",
    "        # Visualize FID\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(epoch + 1), fid_scores, label='FID')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('FID')\n",
    "        plt.title('FID Score')\n",
    "        plt.legend()\n",
    "        os.makedirs(\"cDCGANs-Gray/images/fid\", exist_ok=True)\n",
    "        plt.savefig(f\"cDCGANs-Gray/images/fid/fid{epoch+1}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Print FID score\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] FID: {fid:.4f}\")\n",
    "\n",
    "    if (epoch+1) % 1 == 0 or epoch == 0:\n",
    "        gen.eval()\n",
    "        noise = torch.randn(32, z_dim, 1, 1).to(device)\n",
    "        fake = gen(noise, labels[:32])\n",
    "        img_grid_fake = make_grid(fake[:32], normalize=True)\n",
    "        plt.imshow(img_grid_fake.permute(1, 2, 0).cpu().numpy())\n",
    "        os.makedirs(\"cDCGANs-Gray/images/fake\", exist_ok=True)\n",
    "        plt.savefig(f\"cDCGANs-Gray/images/fake/fake{epoch+1}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # Save evaluation metrics, generator, disc, and optimizers every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        os.makedirs(f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}\", exist_ok=True)\n",
    "        \n",
    "        # Visualize FID\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(epoch + 1), fid_scores, label='FID')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('FID')\n",
    "        plt.title('FID Score')\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/fid_plot.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(losses_disc, label='Disc Loss')\n",
    "        plt.plot(losses_gen, label='Generator Loss')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Epoch {epoch+1}/{epochs}: Disc and Generator Losses')\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/losses_plot.png\")  \n",
    "        \n",
    "        plt.imsave(f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/fake_{epoch+1}.png\", img_grid_fake.permute(1, 2, 0).cpu().numpy())\n",
    "        # Save evaluation metrics\n",
    "        evaluation_metrics = {\"FID\": fid_scores[-1]}\n",
    "        torch.save(evaluation_metrics, f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/evaluation_metrics.pth\")\n",
    "\n",
    "        # Save generator, disc, and optimizers\n",
    "        torch.save(gen.state_dict(), f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/gen_{epoch+1}.pth\")\n",
    "        torch.save(disc.state_dict(), f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/disc_{epoch+1}.pth\")\n",
    "        torch.save(opt_gen.state_dict(), f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/opt_gen_{epoch+1}.pth\")\n",
    "        torch.save(opt_disc.state_dict(), f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/opt_disc_{epoch+1}.pth\")\n",
    "\n",
    "        # Save losses and evaluation scores to files\n",
    "        torch.save(losses_disc, f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/losses_disc.pth\")\n",
    "        torch.save(losses_gen, f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/losses_gen.pth\")\n",
    "        torch.save(fid_scores, f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/fid_scores.pth\")\n",
    "    if (epoch+1) %25 == 0:\n",
    "        clear_output()\n",
    "    gen.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.load_state_dict(torch.load(\"C:\\\\Users\\\\ahksa.LAPTOP-394KBUA4\\\\OneDrive\\\\Desktop\\\\CESS Files\\\\Semester 6\\\\Artificial Intelligence\\\\Project\\\\Image-Generation-Using-Generative-AI\\\\cDCGANs-Gray-Scaled\\\\cDCGANs-Gray\\\\cDCGANs-Gray-Arch-2-350\\\\gen_350.pth\"))\n",
    "disc.load_state_dict(torch.load(\"C:\\\\Users\\\\ahksa.LAPTOP-394KBUA4\\\\OneDrive\\\\Desktop\\\\CESS Files\\\\Semester 6\\\\Artificial Intelligence\\\\Project\\\\Image-Generation-Using-Generative-AI\\\\cDCGANs-Gray-Scaled\\\\cDCGANs-Gray\\\\cDCGANs-Gray-Arch-2-350\\\\disc_350.pth\"))\n",
    "opt_disc.load_state_dict(torch.load(\"C:\\\\Users\\\\ahksa.LAPTOP-394KBUA4\\\\OneDrive\\\\Desktop\\\\CESS Files\\\\Semester 6\\\\Artificial Intelligence\\\\Project\\\\Image-Generation-Using-Generative-AI\\\\cDCGANs-Gray-Scaled\\\\cDCGANs-Gray\\\\cDCGANs-Gray-Arch-2-350\\\\opt_disc_350.pth\"))\n",
    "opt_gen.load_state_dict(torch.load(\"C:\\\\Users\\\\ahksa.LAPTOP-394KBUA4\\\\OneDrive\\\\Desktop\\\\CESS Files\\\\Semester 6\\\\Artificial Intelligence\\\\Project\\\\Image-Generation-Using-Generative-AI\\\\cDCGANs-Gray-Scaled\\\\cDCGANs-Gray\\\\cDCGANs-Gray-Arch-2-350\\\\opt_gen_350.pth\"))\n",
    "loss_disc = torch.load(\"C:\\\\Users\\\\ahksa.LAPTOP-394KBUA4\\\\OneDrive\\\\Desktop\\\\CESS Files\\\\Semester 6\\\\Artificial Intelligence\\\\Project\\\\Image-Generation-Using-Generative-AI\\\\cDCGANs-Gray-Scaled\\\\cDCGANs-Gray\\\\cDCGANs-Gray-Arch-2-350\\\\losses_disc.pth\")\n",
    "loss_gen = torch.load(\"C:\\\\Users\\\\ahksa.LAPTOP-394KBUA4\\\\OneDrive\\\\Desktop\\\\CESS Files\\\\Semester 6\\\\Artificial Intelligence\\\\Project\\\\Image-Generation-Using-Generative-AI\\\\cDCGANs-Gray-Scaled\\\\cDCGANs-Gray\\\\cDCGANs-Gray-Arch-2-350\\\\losses_gen.pth\")\n",
    "fid_scores_prev = torch.load(\"C:\\\\Users\\\\ahksa.LAPTOP-394KBUA4\\\\OneDrive\\\\Desktop\\\\CESS Files\\\\Semester 6\\\\Artificial Intelligence\\\\Project\\\\Image-Generation-Using-Generative-AI\\\\cDCGANs-Gray-Scaled\\\\cDCGANs-Gray\\\\cDCGANs-Gray-Arch-2-350\\\\fid_scores.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 451/500:   0%|          | 0/503 [00:01<?, ?it/s, Loss D=0.00139, Loss G=-0.00128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [451/500] Batch 0/503 Loss D: 0.0014, loss G: -0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 451/500:  20%|        | 100/503 [01:54<07:29,  1.12s/it, Loss D=0.00433, Loss G=-0.00283] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [451/500] Batch 100/503 Loss D: 0.0043, loss G: -0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 451/500:  40%|      | 200/503 [03:48<05:56,  1.18s/it, Loss D=0.00335, Loss G=-0.00334]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [451/500] Batch 200/503 Loss D: 0.0034, loss G: -0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 451/500:  60%|    | 300/503 [05:38<03:30,  1.04s/it, Loss D=0.00167, Loss G=-0.000716] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [451/500] Batch 300/503 Loss D: 0.0017, loss G: -0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 451/500:  80%|  | 400/503 [07:29<02:30,  1.46s/it, Loss D=3.44e-5, Loss G=-1.9e-5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [451/500] Batch 400/503 Loss D: 0.0000, loss G: -0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 451/500:  99%|| 500/503 [16:04<00:10,  3.56s/it, Loss D=1.39e-5, Loss G=-1.05e-9]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [451/500] Batch 500/503 Loss D: 0.0000, loss G: -0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 451/500: 100%|| 503/503 [16:12<00:00,  1.93s/it, Loss D=2.44e-5, Loss G=-1.4e-9]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [451/500] FID: 275.2848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 452/500:   0%|          | 0/503 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m loss_gen\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     31\u001b[0m opt_gen\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 32\u001b[0m losses_gen\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     33\u001b[0m losses_disc\u001b[38;5;241m.\u001b[39mappend(loss_disc\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Update progress bar description\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_epoch = 350\n",
    "step = 0\n",
    "losses_disc = loss_disc\n",
    "losses_gen = loss_gen\n",
    "fid_scores = fid_scores_prev\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    tqdm_train_loader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch_idx, (real, labels) in enumerate(tqdm_train_loader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(batch_size, z_dim, 1, 1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        fake = gen(noise,labels)\n",
    "        \n",
    "        # Train Discriminator: max log(D(real)) + log(1 - D(G(z)))\n",
    "        disc_real = disc(real,labels).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake,labels).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake)/2\n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "            \n",
    "        \n",
    "        ## Train Generator: min -E[disc(gen_fake)]\n",
    "        output = disc(fake, labels).reshape(-1)\n",
    "        loss_gen = -torch.mean(output)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        losses_gen.append(loss_gen.item())\n",
    "        losses_disc.append(loss_disc.item())\n",
    "        # Update progress bar description\n",
    "        tqdm_train_loader.set_postfix({\"Loss D\": loss_disc.item(), \"Loss G\": loss_gen.item()})\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            tqdm_train_loader.write(\n",
    "                f\"Epoch [{epoch+1}/{epochs}] Batch {batch_idx}/{len(train_loader)} Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(noise, labels)\n",
    "                img_grid_real = make_grid(real[:32], normalize=True)\n",
    "                img_grid_fake = make_grid(fake[:32], normalize=True)\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                write_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "                step += 1\n",
    "\n",
    "    # Visualize and print losses after each epoch\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses_disc, label='Disc Loss')\n",
    "    plt.plot(losses_gen, label='Generator Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Epoch {epoch+1}/{epochs}: Disc and Generator Losses')\n",
    "    plt.legend()\n",
    "    os.makedirs(\"cDCGANs-Gray/images/loss\", exist_ok=True)\n",
    "    plt.savefig(f\"cDCGANs-Gray/images/loss/loss{epoch+1}.png\")\n",
    "    plt.close()\n",
    "    # Calculate and visualize FID every epoch\n",
    "    with torch.no_grad():\n",
    "        fake = gen(noise, labels)\n",
    "        fid = calculate_fretchet(real, fake, model)\n",
    "        fid_scores.append(fid)\n",
    "\n",
    "        # Visualize FID\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(epoch + 1), fid_scores, label='FID')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('FID')\n",
    "        plt.title('FID Score')\n",
    "        plt.legend()\n",
    "        os.makedirs(\"cDCGANs-Gray/images/fid\", exist_ok=True)\n",
    "        plt.savefig(f\"cDCGANs-Gray/images/fid/fid{epoch+1}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Print FID score\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] FID: {fid:.4f}\")\n",
    "\n",
    "    if (epoch+1) % 1 == 0 or epoch == 0:\n",
    "        gen.eval()\n",
    "        noise = torch.randn(32, z_dim, 1, 1).to(device)\n",
    "        fake = gen(noise, labels[:32])\n",
    "        img_grid_fake = make_grid(fake[:32], normalize=True)\n",
    "        plt.imshow(img_grid_fake.permute(1, 2, 0).cpu().numpy())\n",
    "        os.makedirs(\"cDCGANs-Gray/images/fake\", exist_ok=True)\n",
    "        plt.savefig(f\"cDCGANs-Gray/images/fake/fake{epoch+1}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # Save evaluation metrics, generator, disc, and optimizers every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        os.makedirs(f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}\", exist_ok=True)\n",
    "        \n",
    "        # Visualize FID\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(epoch + 1), fid_scores, label='FID')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('FID')\n",
    "        plt.title('FID Score')\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/fid_plot.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(losses_disc, label='Disc Loss')\n",
    "        plt.plot(losses_gen, label='Generator Loss')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Epoch {epoch+1}/{epochs}: Disc and Generator Losses')\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/losses_plot.png\")  \n",
    "        \n",
    "        plt.imsave(f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/fake_{epoch+1}.png\", img_grid_fake.permute(1, 2, 0).cpu().numpy())\n",
    "        # Save evaluation metrics\n",
    "        evaluation_metrics = {\"FID\": fid_scores[-1]}\n",
    "        torch.save(evaluation_metrics, f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/evaluation_metrics.pth\")\n",
    "\n",
    "        # Save generator, disc, and optimizers\n",
    "        torch.save(gen.state_dict(), f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/gen_{epoch+1}.pth\")\n",
    "        torch.save(disc.state_dict(), f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/disc_{epoch+1}.pth\")\n",
    "        torch.save(opt_gen.state_dict(), f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/opt_gen_{epoch+1}.pth\")\n",
    "        torch.save(opt_disc.state_dict(), f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/opt_disc_{epoch+1}.pth\")\n",
    "\n",
    "        # Save losses and evaluation scores to files\n",
    "        torch.save(losses_disc, f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/losses_disc.pth\")\n",
    "        torch.save(losses_gen, f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/losses_gen.pth\")\n",
    "        torch.save(fid_scores, f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/fid_scores.pth\")\n",
    "    if (epoch+1) %25 == 0:\n",
    "        clear_output()\n",
    "    gen.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}\", exist_ok=True)\n",
    "        \n",
    "# Visualize FID\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(epoch + 1), fid_scores, label='FID')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('FID')\n",
    "plt.title('FID Score')\n",
    "plt.legend()\n",
    "plt.savefig(f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/fid_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_disc, label='Disc Loss')\n",
    "plt.plot(losses_gen, label='Generator Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Epoch {epoch+1}/{epochs}: Disc and Generator Losses')\n",
    "plt.legend()\n",
    "plt.savefig(f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/losses_plot.png\")  \n",
    "\n",
    "plt.imsave(f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/fake_{epoch+1}.png\", img_grid_fake.permute(1, 2, 0).cpu().numpy())\n",
    "# Save evaluation metrics\n",
    "evaluation_metrics = {\"FID\": fid_scores[-1]}\n",
    "torch.save(evaluation_metrics, f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/evaluation_metrics.pth\")\n",
    "\n",
    "# Save generator, disc, and optimizers\n",
    "torch.save(gen.state_dict(), f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/gen_{epoch+1}.pth\")\n",
    "torch.save(disc.state_dict(), f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/disc_{epoch+1}.pth\")\n",
    "torch.save(opt_gen.state_dict(), f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/opt_gen_{epoch+1}.pth\")\n",
    "torch.save(opt_disc.state_dict(), f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/opt_disc_{epoch+1}.pth\")\n",
    "\n",
    "# Save losses and evaluation scores to files\n",
    "torch.save(losses_disc, f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/losses_disc.pth\")\n",
    "torch.save(losses_gen, f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/losses_gen.pth\")\n",
    "torch.save(fid_scores, f\"cDCGANs-Gray/cDCGANs-Gray-Arch-2-{epoch+1}/fid_scores.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "gen_loaded = Generator(z_dim=100, channels_img=1, features_g=64, num_classes=3, img_size=64, embed_size=100)\n",
    "gen_loaded.load_state_dict(torch.load(\"cDCGANs-Gray-Arch-2-41/gen_41.pth\"))\n",
    "gen_loaded.eval()\n",
    "\n",
    "\n",
    "labels = torch.LongTensor([random.randint(0,2) for i in range(32)])\n",
    "noise = torch.randn(32, 100, 1, 1)\n",
    "fake = gen_loaded(noise, labels)\n",
    "img_grid_fake = make_grid(fake[:32], normalize=True)\n",
    "plt.imshow(img_grid_fake.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples now\n",
    "gen.eval()\n",
    "noise = torch.randn(32, z_dim, 1, 1).to(device)\n",
    "labels = torch.LongTensor([random.randint(0,2) for i in range(32)]).to(device)\n",
    "fake = gen(noise,labels)\n",
    "img_grid_fake = make_grid(fake[:32], normalize=True)\n",
    "plt.imshow(img_grid_fake.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.eval()\n",
    "noise = torch.randn(1, z_dim, 1, 1).to(device)\n",
    "label = torch.LongTensor([2]).to(device)\n",
    "fake = gen(noise,label)\n",
    "img_grid_fake = make_grid(fake[:1], normalize=True)\n",
    "plt.imshow(img_grid_fake.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.eval()\n",
    "noise = torch.randn(1, z_dim, 1, 1).to(device)\n",
    "label = torch.LongTensor([1]).to(device)\n",
    "fake = gen(noise,label)\n",
    "img_grid_fake = make_grid(fake[:1], normalize=True)\n",
    "plt.imshow(img_grid_fake.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.eval()\n",
    "noise = torch.rand(1, z_dim, 1, 1).to(device)\n",
    "label = torch.LongTensor([0]).to(device)\n",
    "fake = gen(noise,label)\n",
    "img_grid_fake = make_grid(fake[:1], normalize=True)\n",
    "plt.imshow(img_grid_fake.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gen.state_dict(), \"Models/cgen_25_epochs-modified-training.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(disc.state_dict(), \"Models/cdisc_25_epochs-modified-training.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DCGAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
